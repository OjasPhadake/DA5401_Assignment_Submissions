{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment 6\n",
        "## Author: Ojas Phadake\n",
        "## Roll No: CH22B007\n",
        "\n",
        "### Summary of Assignment:\n",
        "This assignment makes one apply linear and non-linear regression to impute missing values in a dataset. The effectiveness of the imputation methods will be measured indirectly by assessing the performance of a subsequent classification task, comparing the regression-based approach against simpler imputation strategies."
      ],
      "metadata": {
        "id": "Z3FhRQrcNiS5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part A: Data Preprocessing and Imputation\n",
        "### Load and Prepare Data\n",
        "\n",
        "To aritificially introduce Missing At Random (MAR) values to simulate a real-world scenario with a substantial missing data problem."
      ],
      "metadata": {
        "id": "lT1bcvFIN2dI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, ConfusionMatrixDisplay, RocCurveDisplay\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.impute import KNNImputer\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "BipAk39D-y7H"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = kagglehub.dataset_download(\"uciml/default-of-credit-card-clients-dataset\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)\n",
        "df = pd.read_csv(f\"{path}/UCI_Credit_Card.csv\")\n",
        "print(df.shape)\n",
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GQxV0cAj-2QY",
        "outputId": "fabfeba4-30ce-4081-ee1a-caf2b3c0239c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/uciml/default-of-credit-card-clients-dataset?dataset_version_number=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 0.98M/0.98M [00:00<00:00, 1.39MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/uciml/default-of-credit-card-clients-dataset/versions/1\n",
            "(30000, 25)\n",
            "   ID  LIMIT_BAL  SEX  EDUCATION  MARRIAGE  AGE  PAY_0  PAY_2  PAY_3  PAY_4  \\\n",
            "0   1    20000.0    2          2         1   24      2      2     -1     -1   \n",
            "1   2   120000.0    2          2         2   26     -1      2      0      0   \n",
            "2   3    90000.0    2          2         2   34      0      0      0      0   \n",
            "3   4    50000.0    2          2         1   37      0      0      0      0   \n",
            "4   5    50000.0    1          2         1   57     -1      0     -1      0   \n",
            "\n",
            "   ...  BILL_AMT4  BILL_AMT5  BILL_AMT6  PAY_AMT1  PAY_AMT2  PAY_AMT3  \\\n",
            "0  ...        0.0        0.0        0.0       0.0     689.0       0.0   \n",
            "1  ...     3272.0     3455.0     3261.0       0.0    1000.0    1000.0   \n",
            "2  ...    14331.0    14948.0    15549.0    1518.0    1500.0    1000.0   \n",
            "3  ...    28314.0    28959.0    29547.0    2000.0    2019.0    1200.0   \n",
            "4  ...    20940.0    19146.0    19131.0    2000.0   36681.0   10000.0   \n",
            "\n",
            "   PAY_AMT4  PAY_AMT5  PAY_AMT6  default.payment.next.month  \n",
            "0       0.0       0.0       0.0                           1  \n",
            "1    1000.0       0.0    2000.0                           1  \n",
            "2    1000.0    1000.0    5000.0                           0  \n",
            "3    1100.0    1069.0    1000.0                           0  \n",
            "4    9000.0     689.0     679.0                           0  \n",
            "\n",
            "[5 rows x 25 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9UoKASYdOjH8",
        "outputId": "51f7be8e-5b2e-4c75-f5fe-21859c204199"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['ID', 'LIMIT_BAL', 'SEX', 'EDUCATION', 'MARRIAGE', 'AGE', 'PAY_0',\n",
              "       'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6', 'BILL_AMT1', 'BILL_AMT2',\n",
              "       'BILL_AMT3', 'BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6', 'PAY_AMT1',\n",
              "       'PAY_AMT2', 'PAY_AMT3', 'PAY_AMT4', 'PAY_AMT5', 'PAY_AMT6',\n",
              "       'default.payment.next.month'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(42)\n",
        "\n",
        "# columns to have missing values\n",
        "cols_to_nan = ['AGE', 'BILL_AMT1', 'BILL_AMT2', 'BILL_AMT3']\n",
        "\n",
        "# missing values (between 5–10%)\n",
        "missing_fraction = np.random.uniform(0.05, 0.10)\n",
        "\n",
        "for col in cols_to_nan:\n",
        "    n_missing = int(missing_fraction * len(df))\n",
        "    missing_indices = np.random.choice(df.index, n_missing, replace=False)\n",
        "    df.loc[missing_indices, col] = np.nan\n",
        "\n",
        "# Verify missingness\n",
        "print(df[cols_to_nan].isnull().sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "afJ3Gia6_TEk",
        "outputId": "811eabda-5863-4568-aedf-d1bb70f37b82"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AGE          2061\n",
            "BILL_AMT1    2061\n",
            "BILL_AMT2    2061\n",
            "BILL_AMT3    2061\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we have artificially introduced MAR missing values (roughly 6.8% of the number of values) in the 4 columns given by `AGE`, `BILL_AMT`, `BILL_AMT2` and `BILL_AMT3`. Now we proceed to carry out the imputation strategies.\n",
        "\n",
        "### Imputation Strategy 1: Simple Imputation"
      ],
      "metadata": {
        "id": "Hwk1Y03cPj0m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns[df.isnull().any()]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-7BeH66XRCep",
        "outputId": "88c85e80-0347-425d-b186-3ddbca0dd32f"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['AGE', 'BILL_AMT1', 'BILL_AMT2', 'BILL_AMT3'], dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_A = df.copy()\n",
        "\n",
        "cols_with_missing = df_A.columns[df_A.isnull().any()]\n",
        "\n",
        "# Perform median imputation for each column with missing values\n",
        "for col in cols_with_missing:\n",
        "    median_value = df_A[col].median()\n",
        "    df_A[col].fillna(median_value, inplace=True)\n",
        "\n",
        "# Verify imputation\n",
        "print(\"Missing values after median imputation:\")\n",
        "print(df_A[cols_with_missing].isnull().sum())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3AnaaSe1-64r",
        "outputId": "a9690fe6-d2a6-40a3-c084-2b9735ebffc3"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Missing values after median imputation:\n",
            "AGE          0\n",
            "BILL_AMT1    0\n",
            "BILL_AMT2    0\n",
            "BILL_AMT3    0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Explanation: Why Median Is Often Preferred Over Mean for Imputation**\n",
        "\n",
        "The **median** is often preferred over the **mean** when imputing missing numerical values because:\n",
        "\n",
        "1. **Robustness to Outliers:**\n",
        "   The median is not affected by extreme values. In datasets like the credit card default data, financial features (e.g., `AGE`, `BILL_AMT1`, ...) can have very large outliers due to a few clients with exceptionally high bills or payments.\n",
        "   → Using the mean would **shift the imputed value** toward these extremes, distorting the data distribution.\n",
        "   → The median, on the other hand, provides a **more stable central value**.\n",
        "\n",
        "2. **Preservation of Data Distribution:**\n",
        "   Median imputation tends to preserve the original skewness and variability of the data better than mean imputation, which can artificially reduce variance.\n",
        "\n",
        "3. **More Realistic for Non-Normal Data:**\n",
        "   Many financial and demographic variables (like `AGE`, `BILL_AMTx`) are **not normally distributed**.\n",
        "   In such cases, the median is a **better measure of central tendency** than the mean."
      ],
      "metadata": {
        "id": "amHg6sOPCjO0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Imputation Strategy 2: Regression Imputation (Linear)\n",
        "\n",
        "We wil be selecting the column `AGE` to perform LinearRegression on."
      ],
      "metadata": {
        "id": "nE82Qk9BReU3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_B = df.copy()\n",
        "target_col = 'AGE'\n",
        "\n",
        "# Separate rows with and without missing AGE\n",
        "df_train = df_B[df_B[target_col].notnull()]\n",
        "df_missing = df_B[df_B[target_col].isnull()]\n",
        "\n",
        "# Select numerical predictor columns (excluding the target and dependent variable)\n",
        "predictor_cols = df_B.select_dtypes(include=[np.number]).columns.drop(\n",
        "    [target_col, 'default.payment.next.month']\n",
        ")\n",
        "\n",
        "# Prepare training and prediction sets\n",
        "X_train = df_train[predictor_cols]\n",
        "y_train = df_train[target_col]\n",
        "X_missing = df_missing[predictor_cols]\n",
        "\n",
        "# Handle any missing values in predictors by simple imputation (median)\n",
        "X_train = X_train.fillna(X_train.median())\n",
        "X_missing = X_missing.fillna(X_train.median())\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_missing_scaled = scaler.transform(X_missing)\n",
        "\n",
        "# Train Linear Regression model\n",
        "lin_reg = LinearRegression()\n",
        "lin_reg.fit(X_train_scaled, y_train)\n",
        "\n",
        "predicted_age = lin_reg.predict(X_missing_scaled)\n",
        "\n",
        "# Fill in the predicted values\n",
        "df_B.loc[df_B[target_col].isnull(), target_col] = predicted_age\n",
        "\n",
        "# Verify imputation\n",
        "print(f\"Missing values in '{target_col}' after regression imputation:\",\n",
        "      df_B[target_col].isnull().sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7VTz5oTQCgfr",
        "outputId": "0008a52e-5bde-4d90-bffe-92415eb4f648"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Missing values in 'AGE' after regression imputation: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explanation behind Linear Regression Imputation\n",
        "\n",
        "**Underlying Assumption: Missing At Random (MAR)**\n",
        "\n",
        "The **Regression Imputation** method assumes that the missing data mechanism is **Missing At Random (MAR)**.\n",
        "\n",
        "#### What MAR means:\n",
        "\n",
        "> A variable is *Missing At Random* if the probability that a value is missing **depends only on other observed variables** in the dataset, **not on the missing value itself**.\n",
        "\n",
        "Formally,\n",
        "$$ P(\\text{Missing in X} \\mid X, Y) = P(\\text{Missing in X} \\mid Y) $$\n",
        "where (Y) are the observed variables.\n",
        "\n",
        "That is, after controlling for (Y), missingness in (X) is independent of the true (unobserved) values of (X).\n",
        "\n",
        "---\n",
        "\n",
        "### **Example in this dataset**\n",
        "\n",
        "The column **`AGE`** has some missing values.\n",
        "\n",
        "* Under the **MAR assumption**, the fact that a person’s **AGE** is missing could be related to other observed information — for example:\n",
        "\n",
        "  * Their **education level**, **bill amount**, or **payment history**.\n",
        "* But it is **not directly related to their actual age value** (the missing value itself).\n",
        "\n",
        "So, if two clients have the same observed data (same `LIMIT_BAL`, `BILL_AMT1`, etc.), they should have the **same probability** of having their `AGE` missing — regardless of what their true age is.\n",
        "\n",
        "---\n",
        "\n",
        "### **Why Regression Imputation Relies on MAR**\n",
        "\n",
        "Regression imputation uses other **observed variables** to predict the missing ones.\n",
        "This approach only works well if those observed variables truly capture the pattern behind the missingness.\n",
        "\n",
        "If the missingness instead depends on the **unobserved value itself** — i.e., data are **Missing Not At Random (MNAR)** — then regression imputation will produce **biased estimates** because the model cannot account for the hidden dependency.\n",
        "\n",
        "---\n",
        "\n",
        "### **Summary**\n",
        "\n",
        "| Missingness Type                        | Description                                         | Can Regression Imputation Handle It? |\n",
        "| --------------------------------------- | --------------------------------------------------- | ------------------------------------ |\n",
        "| **MCAR** (Missing Completely At Random) | Missingness is random and unrelated to any variable | ✅ Works fine                         |\n",
        "| **MAR** (Missing At Random)             | Missingness depends on observed data                | ✅ Works well (assumed case)          |\n",
        "| **MNAR** (Missing Not At Random)        | Missingness depends on the missing value itself     | ❌ Leads to bias                      |\n"
      ],
      "metadata": {
        "id": "3uhnM3BsD7Wj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a clean dataset copy\n",
        "df_C = df.copy()\n",
        "\n",
        "# Column to impute\n",
        "target_col = 'AGE'\n",
        "target_variable = 'default.payment.next.month'\n",
        "\n",
        "# Separate rows with and without missing AGE\n",
        "df_train = df_C[df_C[target_col].notnull()]\n",
        "df_missing = df_C[df_C[target_col].isnull()]\n",
        "\n",
        "# Select predictor columns\n",
        "predictor_cols = df_C.select_dtypes(include=[np.number]).columns.drop([target_col, target_variable])\n",
        "\n",
        "# Prepare train/test sets\n",
        "X_train = df_train[predictor_cols]\n",
        "y_train = df_train[target_col]\n",
        "X_missing = df_missing[predictor_cols]\n",
        "\n",
        "# Handle missing predictor values with median\n",
        "X_train = X_train.fillna(X_train.median())\n",
        "X_missing = X_missing.fillna(X_train.median())\n",
        "\n",
        "# Standardize predictors\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_missing_scaled = scaler.transform(X_missing)\n",
        "\n",
        "# Train KNN Regressor\n",
        "knn_reg = KNeighborsRegressor(n_neighbors=5, weights='distance')\n",
        "knn_reg.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predict missing AGE values\n",
        "predicted_age_knn = knn_reg.predict(X_missing_scaled)\n",
        "\n",
        "# Fill missing AGE values in dataset\n",
        "df_C.loc[df_C[target_col].isnull(), target_col] = predicted_age_knn\n",
        "\n",
        "# Verify completion\n",
        "print(f\"Missing values in '{target_col}' after KNN regression imputation:\",\n",
        "      df_C[target_col].isnull().sum())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cW7x3de3C_kq",
        "outputId": "7b9ff84e-3b20-4494-a9a4-1b03ad324b6d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Missing values in 'AGE' after KNN regression imputation: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part B: Model Training and Performance Assessment\n",
        "### Data Split\n",
        "\n",
        "Let us now split the training sets of the models A,B,C and D into training and testing sets."
      ],
      "metadata": {
        "id": "7tuwYcAHVdzA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "target_variable = 'default.payment.next.month'\n",
        "\n",
        "# -------------------------------\n",
        "# Dataset A: Median Imputation\n",
        "# -------------------------------\n",
        "X_A = df_A.drop(columns=[target_variable])\n",
        "y_A = df_A[target_variable]\n",
        "\n",
        "X_train_A, X_test_A, y_train_A, y_test_A = train_test_split(\n",
        "    X_A, y_A, test_size=0.2, random_state=42, stratify=y_A\n",
        ")\n",
        "\n",
        "# -------------------------------\n",
        "# Dataset B: Linear Regression Imputation\n",
        "# -------------------------------\n",
        "X_B = df_B.drop(columns=[target_variable])\n",
        "y_B = df_B[target_variable]\n",
        "\n",
        "X_train_B, X_test_B, y_train_B, y_test_B = train_test_split(\n",
        "    X_B, y_B, test_size=0.2, random_state=42, stratify=y_B\n",
        ")\n",
        "\n",
        "# -------------------------------\n",
        "# Dataset C: Non-linear Regression (KNN) Imputation\n",
        "# -------------------------------\n",
        "X_C = df_C.drop(columns=[target_variable])\n",
        "y_C = df_C[target_variable]\n",
        "\n",
        "X_train_C, X_test_C, y_train_C, y_test_C = train_test_split(\n",
        "    X_C, y_C, test_size=0.2, random_state=42, stratify=y_C\n",
        ")\n",
        "\n",
        "# -------------------------------\n",
        "# Dataset D: Listwise Deletion (remove any rows with NaN)\n",
        "# -------------------------------\n",
        "df_D = df.dropna().copy()\n",
        "\n",
        "X_D = df_D.drop(columns=[target_variable])\n",
        "y_D = df_D[target_variable]\n",
        "\n",
        "X_train_D, X_test_D, y_train_D, y_test_D = train_test_split(\n",
        "    X_D, y_D, test_size=0.2, random_state=42, stratify=y_D\n",
        ")\n",
        "\n",
        "# -------------------------------\n",
        "# Verify shapes of splits\n",
        "# -------------------------------\n",
        "print(\"Dataset A (Median Imputation):\", X_train_A.shape, X_test_A.shape)\n",
        "print(\"Dataset B (Linear Imputation):\", X_train_B.shape, X_test_B.shape)\n",
        "print(\"Dataset C (Non-linear Imputation):\", X_train_C.shape, X_test_C.shape)\n",
        "print(\"Dataset D (Listwise Deletion):\", X_train_D.shape, X_test_D.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hb0-pmUWEIsU",
        "outputId": "fca091fa-8ece-4715-be83-4c49aa41a45e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset A (Median Imputation): (24000, 24) (6000, 24)\n",
            "Dataset B (Linear Imputation): (24000, 24) (6000, 24)\n",
            "Dataset C (Non-linear Imputation): (24000, 24) (6000, 24)\n",
            "Dataset D (Listwise Deletion): (18047, 24) (4512, 24)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Classifier Setup\n",
        "\n",
        "Standardize the dataset using StandardScaler() from sklearn library."
      ],
      "metadata": {
        "id": "qvn2xPwpVjvn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create separate scalers for each dataset\n",
        "scaler_A = StandardScaler()\n",
        "scaler_B = StandardScaler()\n",
        "scaler_C = StandardScaler()\n",
        "scaler_D = StandardScaler()\n",
        "\n",
        "# -------------------------------\n",
        "# Dataset A\n",
        "# -------------------------------\n",
        "X_train_A_scaled = scaler_A.fit_transform(X_train_A)\n",
        "X_test_A_scaled = scaler_A.transform(X_test_A)\n",
        "\n",
        "# -------------------------------\n",
        "# Dataset B\n",
        "# -------------------------------\n",
        "X_train_B_scaled = scaler_B.fit_transform(X_train_B)\n",
        "X_test_B_scaled = scaler_B.transform(X_test_B)\n",
        "\n",
        "# -------------------------------\n",
        "# Dataset C\n",
        "# -------------------------------\n",
        "X_train_C_scaled = scaler_C.fit_transform(X_train_C)\n",
        "X_test_C_scaled = scaler_C.transform(X_test_C)\n",
        "\n",
        "# -------------------------------\n",
        "# Dataset D\n",
        "# -------------------------------\n",
        "X_train_D_scaled = scaler_D.fit_transform(X_train_D)\n",
        "X_test_D_scaled = scaler_D.transform(X_test_D)\n",
        "\n",
        "# -------------------------------\n",
        "# Verification\n",
        "# -------------------------------\n",
        "print(\"Feature standardization complete for all four datasets.\")\n",
        "print(\"Shapes (train/test):\")\n",
        "print(\"A:\", X_train_A_scaled.shape, X_test_A_scaled.shape)\n",
        "print(\"B:\", X_train_B_scaled.shape, X_test_B_scaled.shape)\n",
        "print(\"C:\", X_train_C_scaled.shape, X_test_C_scaled.shape)\n",
        "print(\"D:\", X_train_D_scaled.shape, X_test_D_scaled.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "raJNkn_lEq8q",
        "outputId": "7d9d3667-cc86-44b1-94a9-bf246064af8a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature standardization complete for all four datasets.\n",
            "Shapes (train/test):\n",
            "A: (24000, 24) (6000, 24)\n",
            "B: (24000, 24) (6000, 24)\n",
            "C: (24000, 24) (6000, 24)\n",
            "D: (18047, 24) (4512, 24)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_features(X_train, X_test):\n",
        "    # Fill any remaining NaNs using median\n",
        "    imputer = SimpleImputer(strategy='median')\n",
        "    X_train_imputed = imputer.fit_transform(X_train)\n",
        "    X_test_imputed = imputer.transform(X_test)\n",
        "\n",
        "    # Standardize features\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train_imputed)\n",
        "    X_test_scaled = scaler.transform(X_test_imputed)\n",
        "\n",
        "    return X_train_scaled, X_test_scaled\n",
        "\n",
        "# Preprocess all datasets\n",
        "X_train_A_scaled, X_test_A_scaled = preprocess_features(X_train_A, X_test_A)\n",
        "X_train_B_scaled, X_test_B_scaled = preprocess_features(X_train_B, X_test_B)\n",
        "X_train_C_scaled, X_test_C_scaled = preprocess_features(X_train_C, X_test_C)\n",
        "X_train_D_scaled, X_test_D_scaled = preprocess_features(X_train_D, X_test_D)\n"
      ],
      "metadata": {
        "id": "-Od2-0LDFQgM"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Evaluation\n",
        "\n",
        "Train `LogisticRegression` on each of the training dataset and use it to predict on the corresponding test dataset."
      ],
      "metadata": {
        "id": "Nc8aLsQSVoyS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to train and evaluate Logistic Regression\n",
        "def train_and_evaluate(X_train, X_test, y_train, y_test, label):\n",
        "    print(f\"\\n===== Logistic Regression Results for {label} =====\")\n",
        "    model = LogisticRegression(max_iter=1000, random_state=42)\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Train and evaluate on all datasets\n",
        "train_and_evaluate(X_train_A_scaled, X_test_A_scaled, y_train_A, y_test_A, \"Dataset A (Mean Imputation)\")\n",
        "train_and_evaluate(X_train_B_scaled, X_test_B_scaled, y_train_B, y_test_B, \"Dataset B (Linear Regression Imputation)\")\n",
        "train_and_evaluate(X_train_C_scaled, X_test_C_scaled, y_train_C, y_test_C, \"Dataset C (Non-linear Regression Imputation)\")\n",
        "train_and_evaluate(X_train_D_scaled, X_test_D_scaled, y_train_D, y_test_D, \"Dataset D (Listwise Deletion)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2y2qaDsmEy4Z",
        "outputId": "c4e98ef6-24ff-408f-bc94-758b40d2500f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Logistic Regression Results for Dataset A (Mean Imputation) =====\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.97      0.89      4673\n",
            "           1       0.69      0.24      0.35      1327\n",
            "\n",
            "    accuracy                           0.81      6000\n",
            "   macro avg       0.75      0.60      0.62      6000\n",
            "weighted avg       0.79      0.81      0.77      6000\n",
            "\n",
            "\n",
            "===== Logistic Regression Results for Dataset B (Linear Regression Imputation) =====\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.97      0.89      4673\n",
            "           1       0.69      0.24      0.35      1327\n",
            "\n",
            "    accuracy                           0.81      6000\n",
            "   macro avg       0.75      0.60      0.62      6000\n",
            "weighted avg       0.79      0.81      0.77      6000\n",
            "\n",
            "\n",
            "===== Logistic Regression Results for Dataset C (Non-linear Regression Imputation) =====\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.97      0.89      4673\n",
            "           1       0.68      0.24      0.35      1327\n",
            "\n",
            "    accuracy                           0.81      6000\n",
            "   macro avg       0.75      0.60      0.62      6000\n",
            "weighted avg       0.79      0.81      0.77      6000\n",
            "\n",
            "\n",
            "===== Logistic Regression Results for Dataset D (Listwise Deletion) =====\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.97      0.89      3511\n",
            "           1       0.73      0.27      0.39      1001\n",
            "\n",
            "    accuracy                           0.82      4512\n",
            "   macro avg       0.78      0.62      0.64      4512\n",
            "weighted avg       0.80      0.82      0.78      4512\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part C: Comparative Analysis\n",
        "### Results Comparison\n",
        "\n",
        "Given below is a comparitive analysis between the results and various metrics for all the 4 methods, followed by a comprehensive discussion about the efficacy discussion."
      ],
      "metadata": {
        "id": "CQOzo9sqV_5J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to extract metrics into a dictionary\n",
        "def get_metrics(y_true, y_pred):\n",
        "    report = classification_report(y_true, y_pred, output_dict=True)\n",
        "    # We take the 'weighted avg' row to summarize performance across both classes\n",
        "    metrics = {\n",
        "        'Accuracy': report['accuracy'],\n",
        "        'Precision': report['weighted avg']['precision'],\n",
        "        'Recall': report['weighted avg']['recall'],\n",
        "        'F1-score': report['weighted avg']['f1-score']\n",
        "    }\n",
        "    return metrics\n",
        "\n",
        "# Train models and collect metrics\n",
        "def train_and_collect_metrics(X_train, X_test, y_train, y_test):\n",
        "    model = LogisticRegression(max_iter=1000, random_state=42)\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    return get_metrics(y_test, y_pred)\n",
        "\n",
        "# Collect metrics for all datasets\n",
        "metrics_A = train_and_collect_metrics(X_train_A_scaled, X_test_A_scaled, y_train_A, y_test_A)\n",
        "metrics_B = train_and_collect_metrics(X_train_B_scaled, X_test_B_scaled, y_train_B, y_test_B)\n",
        "metrics_C = train_and_collect_metrics(X_train_C_scaled, X_test_C_scaled, y_train_C, y_test_C)\n",
        "metrics_D = train_and_collect_metrics(X_train_D_scaled, X_test_D_scaled, y_train_D, y_test_D)\n",
        "\n",
        "# Create summary DataFrame\n",
        "import pandas as pd\n",
        "\n",
        "summary_df = pd.DataFrame([metrics_A, metrics_B, metrics_C, metrics_D],\n",
        "                          index=['Model A (Median Imputation)',\n",
        "                                 'Model B (Linear Regression Imputation)',\n",
        "                                 'Model C (Non-linear Regression Imputation)',\n",
        "                                 'Model D (Listwise Deletion)'])\n",
        "\n",
        "# Display summary table\n",
        "print(\"===== Summary of Model Performance =====\")\n",
        "print(summary_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YjyKJFVlE5Sc",
        "outputId": "5cd873b5-a955-4331-efbf-90ba17c2b358"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===== Summary of Model Performance =====\n",
            "                                            Accuracy  Precision    Recall  \\\n",
            "Model A (Median Imputation)                 0.807667   0.789085  0.807667   \n",
            "Model B (Linear Regression Imputation)      0.807333   0.788280  0.807333   \n",
            "Model C (Non-linear Regression Imputation)  0.807333   0.788192  0.807333   \n",
            "Model D (Listwise Deletion)                 0.815824   0.803023  0.815824   \n",
            "\n",
            "                                            F1-score  \n",
            "Model A (Median Imputation)                 0.768959  \n",
            "Model B (Linear Regression Imputation)      0.768845  \n",
            "Model C (Non-linear Regression Imputation)  0.768987  \n",
            "Model D (Listwise Deletion)                 0.780685  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Efficacy Discussion"
      ],
      "metadata": {
        "id": "bl0t9PeOWGSF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1. Trade-off Between Listwise Deletion (Model D) and Imputation (Models A, B, C)**\n",
        "\n",
        "**Listwise Deletion (Model D):**\n",
        "\n",
        "* **Removes** all rows containing missing values, ensuring the dataset used for training is completely clean.\n",
        "* **Pros:**\n",
        "\n",
        "  * Simplifies model training since no missing data remain.\n",
        "  * Avoids potential bias introduced by incorrect imputation if the data were **Missing Completely At Random (MCAR)**.\n",
        "* **Cons:**\n",
        "\n",
        "  * **Reduces dataset size**, leading to a loss of potentially valuable information.\n",
        "  * Decreases **statistical power** and can result in biased estimates if missingness is related to other observed variables (MAR).\n",
        "\n",
        "**Imputation Methods (Models A, B, C):**\n",
        "\n",
        "* Replace missing values with estimated ones, preserving the number of samples.\n",
        "* **Pros:**\n",
        "\n",
        "  * Retains all data points, helping the model learn from the full dataset.\n",
        "  * Regression-based imputations (Models B and C) use relationships between features to produce more realistic estimates.\n",
        "* **Cons:**\n",
        "\n",
        "  * May introduce small inaccuracies or bias if the imputation assumptions are violated.\n",
        "  * Simple median imputation (Model A) ignores relationships between features, leading to less precise replacements.\n",
        "\n",
        "**Observed Results:**\n",
        "Although **Model D** (Listwise Deletion) achieved the **highest F1-score (0.7807)** and **accuracy (0.8158)**, this result does not necessarily mean deletion is always superior.\n",
        "The performance gain might reflect that the deleted subset contained noisier or inconsistent samples. However, in real-world cases, listwise deletion can **drastically reduce dataset size**, which typically harms model generalization — especially when missingness is widespread.\n",
        "\n",
        "In contrast, **Models A, B, and C** (with F1-scores around **0.769**) preserved all samples, providing a more robust and complete training base, even if their short-term performance is slightly lower.\n",
        "\n",
        "---\n",
        "\n",
        "### **2. Linear vs. Non-Linear Regression Imputation (Models B vs. C)**\n",
        "\n",
        "**Observation:**\n",
        "\n",
        "* Model B (Linear Regression Imputation): F1-score = **0.7688**\n",
        "* Model C (Non-linear Regression Imputation): F1-score = **0.7690**\n",
        "\n",
        "**Interpretation:**\n",
        "\n",
        "* The difference between the two is **minimal**, but the **non-linear regression model (C)** performed slightly better.\n",
        "* This small improvement indicates that the relationship between the imputed feature (e.g., `AGE`) and predictors (`BILL_AMT1–6`, `LIMIT_BAL`) is **not strictly linear**.\n",
        "* Non-linear models (like KNN or Decision Tree regression) capture **non-linear dependencies and local variations** better than linear models, which assume a straight-line relationship between predictors and the target.\n",
        "\n",
        "**Conceptual Link:**\n",
        "This aligns with the **Missing At Random (MAR)** assumption — the missing values depend on other observed features (like billing amounts), and modeling those relationships flexibly helps produce more accurate imputations.\n",
        "\n",
        "---\n",
        "\n",
        "### **3. Recommendation and Conclusion**\n",
        "\n",
        "| Strategy                                       | Pros                                                 | Cons                                    | F1-score (Observed) |\n",
        "| ---------------------------------------------- | ---------------------------------------------------- | --------------------------------------- | ------------------- |\n",
        "| **Median Imputation (Model A)**                | Simple, fast, robust to outliers                     | Ignores feature relationships           | 0.7689              |\n",
        "| **Linear Regression Imputation (Model B)**     | Utilizes linear correlations, preserves variance     | Cannot capture non-linear dependencies  | 0.7688              |\n",
        "| **Non-linear Regression Imputation (Model C)** | Captures complex interactions, realistic imputations | Slightly more computationally intensive | 0.7690              |\n",
        "| **Listwise Deletion (Model D)**                | Simplifies model, removes noise                      | Data loss, possible bias                | 0.7807              |\n",
        "\n",
        "**Final Recommendation:**\n",
        "While **Listwise Deletion (Model D)** achieved the best numerical performance here, it is **not a generally reliable strategy** — its success likely stems from reduced noise after dropping incomplete rows rather than a fundamentally better approach.\n",
        "For most real-world scenarios with significant missingness, **Non-linear Regression Imputation (Model C)** is the **most balanced and conceptually sound** method:\n",
        "\n",
        "* It **preserves all data** and avoids information loss.\n",
        "* It **models complex dependencies** between variables, improving imputation quality.\n",
        "* It aligns with the **MAR assumption**, ensuring that missing values are replaced based on meaningful observed relationships.\n",
        "\n",
        "Thus, the recommended approach for handling missing data in this context is **non-linear regression imputation**, as it provides a strong balance between completeness, theoretical robustness, and predictive accuracy."
      ],
      "metadata": {
        "id": "ej9cT15sGE5n"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "55nQPzKXFw_j"
      },
      "execution_count": 24,
      "outputs": []
    }
  ]
}